{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80d9ada7",
   "metadata": {},
   "source": [
    "# Set Up Your Environment\n",
    "Create a new SageMaker session using the following code block. This returns the role ARN for the session. This role ARN should be the execution role ARN that you set up as a prerequisite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e794ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import sagemaker.session\n",
    "\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "model_package_group_name = f\"BankCDPackageGroupName\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c7d827",
   "metadata": {},
   "source": [
    "Download the dataset into your account's default Amazon S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af2f567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-256555058276/BankCD/bank_clean.csv\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data\n",
    "local_path = \"data/bank_clean.csv\"\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "# s3.Bucket(f\"sagemaker-servicecatalog-seedcode-{region}\").download_file(\n",
    "#     \"dataset/abalone-dataset.csv\",\n",
    "#     local_path\n",
    "# )\n",
    "\n",
    "base_uri = f\"s3://{default_bucket}/BankCD\"\n",
    "input_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=local_path, \n",
    "    desired_s3_uri=base_uri,\n",
    ")\n",
    "print(input_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356e8a10",
   "metadata": {},
   "source": [
    "Download a second dataset for batch transformation after your model is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e215b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_path = \"data/abalone-dataset-batch\"\n",
    "\n",
    "# s3 = boto3.resource(\"s3\")\n",
    "# s3.Bucket(f\"sagemaker-servicecatalog-seedcode-{region}\").download_file(\n",
    "#     \"dataset/abalone-dataset-batch\",\n",
    "#     local_path\n",
    "# )\n",
    "\n",
    "# base_uri = f\"s3://{default_bucket}/abalone\"\n",
    "# batch_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "#     local_path=local_path, \n",
    "#     desired_s3_uri=base_uri,\n",
    "# )\n",
    "# print(batch_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a9e0b8",
   "metadata": {},
   "source": [
    "# Step 2: Define Pipeline Parameters\n",
    "This code block defines the following parameters for your pipeline:\n",
    "\n",
    "- processing_instance_count – The instance count of the processing job.\n",
    "\n",
    "- input_data – The Amazon S3 location of the input data.\n",
    "\n",
    "- batch_data – The Amazon S3 location of the input data for batch transformation.\n",
    "\n",
    "- model_approval_status – The approval status to register the trained model with for CI/CD. For more information, see Automate MLOps with SageMaker Projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e3e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"PendingManualApproval\"\n",
    ")\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=input_data_uri,\n",
    ")\n",
    "batch_data = ParameterString(\n",
    "    name=\"BatchData\",\n",
    "    default_value=batch_data_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd31d698",
   "metadata": {},
   "source": [
    "# Step 3: Define a Processing Step for Feature Engineering\n",
    "This section shows how to create a processing step to prepare the data from the dataset for training.\n",
    "\n",
    "To create a processing step\n",
    "\n",
    "1. Create a directory for the processing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dc78f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p BankCdProcess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99fe993",
   "metadata": {},
   "source": [
    "2. Create a file in the /abalone directory named preprocessing.py with the following content. This preprocessing script is passed in to the processing step for execution on the input data. The training step then uses the preprocessed training features and labels to train a model, and the evaluation step uses the trained model and preprocessed test features and labels to evaluate the model. The script uses scikit-learn to do the following:\n",
    "\n",
    "- Fill in missing sex categorical data and encode it so it's suitable for training.\n",
    "\n",
    "- Scale and normalize all numerical fields except for rings and sex.\n",
    "\n",
    "- Split the data into training, test, and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ec09e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile abalone/preprocessing.py\n",
    "# import argparse\n",
    "# import os\n",
    "# import requests\n",
    "# import tempfile\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "# # Because this is a headerless CSV file, specify the column names here.\n",
    "# feature_columns_names = [\n",
    "#     \"sex\",\n",
    "#     \"length\",\n",
    "#     \"diameter\",\n",
    "#     \"height\",\n",
    "#     \"whole_weight\",\n",
    "#     \"shucked_weight\",\n",
    "#     \"viscera_weight\",\n",
    "#     \"shell_weight\",\n",
    "# ]\n",
    "# label_column = \"rings\"\n",
    "\n",
    "# feature_columns_dtype = {\n",
    "#     \"sex\": str,\n",
    "#     \"length\": np.float64,\n",
    "#     \"diameter\": np.float64,\n",
    "#     \"height\": np.float64,\n",
    "#     \"whole_weight\": np.float64,\n",
    "#     \"shucked_weight\": np.float64,\n",
    "#     \"viscera_weight\": np.float64,\n",
    "#     \"shell_weight\": np.float64\n",
    "# }\n",
    "# label_column_dtype = {\"rings\": np.float64}\n",
    "\n",
    "\n",
    "# def merge_two_dicts(x, y):\n",
    "#     z = x.copy()\n",
    "#     z.update(y)\n",
    "#     return z\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     base_dir = \"/opt/ml/processing\"\n",
    "\n",
    "#     df = pd.read_csv(\n",
    "#         f\"{base_dir}/input/abalone-dataset.csv\",\n",
    "#         header=None, \n",
    "#         names=feature_columns_names + [label_column],\n",
    "#         dtype=merge_two_dicts(feature_columns_dtype, label_column_dtype)\n",
    "#     )\n",
    "#     numeric_features = list(feature_columns_names)\n",
    "#     numeric_features.remove(\"sex\")\n",
    "#     numeric_transformer = Pipeline(\n",
    "#         steps=[\n",
    "#             (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "#             (\"scaler\", StandardScaler())\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     categorical_features = [\"sex\"]\n",
    "#     categorical_transformer = Pipeline(\n",
    "#         steps=[\n",
    "#             (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "#             (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     preprocess = ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             (\"num\", numeric_transformer, numeric_features),\n",
    "#             (\"cat\", categorical_transformer, categorical_features)\n",
    "#         ]\n",
    "#     )\n",
    "    \n",
    "#     y = df.pop(\"rings\")\n",
    "#     X_pre = preprocess.fit_transform(df)\n",
    "#     y_pre = y.to_numpy().reshape(len(y), 1)\n",
    "    \n",
    "#     X = np.concatenate((y_pre, X_pre), axis=1)\n",
    "    \n",
    "#     np.random.shuffle(X)\n",
    "#     train, validation, test = np.split(X, [int(.7*len(X)), int(.85*len(X))])\n",
    "\n",
    "    \n",
    "#     pd.DataFrame(train).to_csv(f\"{base_dir}/train/train.csv\", header=False, index=False)\n",
    "#     pd.DataFrame(validation).to_csv(f\"{base_dir}/validation/validation.csv\", header=False, index=False)\n",
    "#     pd.DataFrame(test).to_csv(f\"{base_dir}/test/test.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838cfde",
   "metadata": {},
   "source": [
    "3. Create an instance of an SKLearnProcessor to pass in to the processing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fc1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"sklearn-abalone-process\",\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304e4bfe",
   "metadata": {},
   "source": [
    "4. Create a processing step. This step takes in the SKLearnProcessor, the input and output channels, and the preprocessing.py script that you created. This is very similar to a processor instance's run method in the SageMaker Python SDK. The input_data parameter passed into ProcessingStep is the input data of the step itself. This input data is used by the processor instance when it runs.\n",
    "\n",
    "Note the  \"train, \"validation, and \"test\" named channels specified in the output configuration for the processing job. Step Properties such as these can be used in subsequent steps and resolve to their runtime values at execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12d2129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "    \n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"AbaloneProcess\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "      ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),  \n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\")\n",
    "    ],\n",
    "    code=\"abalone/preprocessing.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6722a0",
   "metadata": {},
   "source": [
    "# Step 4: Define a Training step\n",
    "This section shows how to use the SageMaker XGBoost Algorithm to train a logistic regression model on the training data output from the processing steps.\n",
    "\n",
    "To define a training step\n",
    "\n",
    "1. Specify the model path where you want to save the models from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d625e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"s3://sagemaker-us-east-1-256555058276/AbaloneTrain\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e6531",
   "metadata": {},
   "source": [
    "2. Configure an estimator for the XGBoost algorithm and the input dataset. The training instance type is passed into the estimator. A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to model_dir so that it can be hosted later. SageMaker uploads the model to Amazon S3 in the form of a model.tar.gz at the end of the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3320e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.0-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=\"ml.m5.xlarge\"\n",
    ")\n",
    "xgb_train = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    output_path=model_path,\n",
    "    role=role,\n",
    ")\n",
    "xgb_train.set_hyperparameters(\n",
    "    objective=\"reg:linear\",\n",
    "    num_round=50,\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.7,\n",
    "    silent=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5f615d",
   "metadata": {},
   "source": [
    "3. Create a TrainingStep using the estimator instance and properties of the ProcessingStep. In particular, pass in the S3Uri of the \"train\" and \"validation\" output channel to the TrainingStep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145d5ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"AbaloneTrain\",\n",
    "    estimator=xgb_train,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"validation\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11c7ea4",
   "metadata": {},
   "source": [
    "# Step 5: Define a Processing Step for Model Evaluation\n",
    "This section shows how to create a processing step to evaluate the accuracy of the model. The result of this model evaluation is used in the condition step to determine which execute path to take.\n",
    "\n",
    "To define a processing step for model evaluation\n",
    "\n",
    "1. Create a file in the /abalone directory named evaluation.py. This script is used in a processing step to perform model evaluation. It takes a trained model and the test dataset as input, then produces a JSON file containing classification evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59b7013",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile abalone/evaluation.py\n",
    "import json\n",
    "import pathlib\n",
    "import pickle\n",
    "import tarfile\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = f\"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "    \n",
    "    model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
    "\n",
    "    test_path = \"/opt/ml/processing/test/test.csv\"\n",
    "    df = pd.read_csv(test_path, header=None)\n",
    "    \n",
    "    y_test = df.iloc[:, 0].to_numpy()\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    \n",
    "    X_test = xgboost.DMatrix(df.values)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    std = np.std(y_test - predictions)\n",
    "    report_dict = {\n",
    "        \"regression_metrics\": {\n",
    "            \"mse\": {\n",
    "                \"value\": mse,\n",
    "                \"standard_deviation\": std\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4be983a",
   "metadata": {},
   "source": [
    "2. Create an instance of a ScriptProcessor that is used to create a ProcessingStep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb1bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "\n",
    "script_eval = ScriptProcessor(\n",
    "    image_uri=image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    base_job_name=\"script-abalone-eval\",\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e883712b",
   "metadata": {},
   "source": [
    "3. Create a ProcessingStep using the processor instance, the input and output channels, and the  evaluation.py script. In particular, pass in the S3ModelArtifacts property from the step_train training step, as well as the S3Uri of the \"test\" output channel of the step_process processing step. This is very similar to a processor instance's run method in the SageMaker Python SDK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bdb25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\"\n",
    ")\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"AbaloneEval\",\n",
    "    processor=script_eval,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"abalone/evaluation.py\",\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0620a74",
   "metadata": {},
   "source": [
    "# Step 6: Define a CreateModelStep for Batch Transformation\n",
    "This section shows how to create a SageMaker model from the output of the training step. This model is used for batch transformation on a new dataset. This step is passed into the condition step and only executes if the condition step evaluates to true.\n",
    "\n",
    "To define a CreateModelStep for batch transformation\n",
    "\n",
    "1. Create a SageMaker model. Pass in the S3ModelArtifacts property from the step_train training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1526b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d642dd56",
   "metadata": {},
   "source": [
    "2. Define the model input for your SageMaker model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b588491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import CreateModelInput\n",
    "\n",
    "\n",
    "inputs = CreateModelInput(\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    accelerator_type=\"ml.eia1.medium\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb18d44",
   "metadata": {},
   "source": [
    "3. Create your CreateModelStep using the CreateModelInput and SageMaker model instance you defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c77a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "\n",
    "step_create_model = CreateModelStep(\n",
    "    name=\"AbaloneCreateModel\",\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc28df3f",
   "metadata": {},
   "source": [
    "# Step 7: Define a TransformStep to Perform Batch Transformation\n",
    "This section shows how to create a TransformStep to perform batch transformation on a dataset after the model is trained. This step is passed into the condition step and only executes if the condition step evaluates to true.\n",
    "\n",
    "To define a TransformStep to perform batch transformation\n",
    "\n",
    "1. Create a transformer instance with the appropriate compute instance type, instance count, and desired output Amazon S3 bucket URI. Pass in the ModelName property from the step_create_model CreateModel step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b21d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    output_path=f\"s3://{default_bucket}/AbaloneTransform\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c59e103",
   "metadata": {},
   "source": [
    "2. Create a TransformStep using the transformer instance you defined and the batch_data pipeline parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951cc03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "\n",
    "\n",
    "step_transform = TransformStep(\n",
    "    name=\"AbaloneTransform\",\n",
    "    transformer=transformer,\n",
    "    inputs=TransformInput(data=batch_data)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682c6214",
   "metadata": {},
   "source": [
    "# Step 8: Define a RegisterModel Step to Create a Model Package\n",
    "This section shows how to construct an instance of RegisterModel. The result of executing RegisterModel in a pipeline is a model package. A model package is a reusable model artifacts abstraction that packages all ingredients necessary for inference. It consists of an inference specification that defines the inference image to use along with an optional model weights location. A model package group is a collection of model packages. You can use a ModelPackageGroup for SageMaker Pipelines to add a new version and model package to the group for every pipeline execution. For more information about model registry, see Register and Deploy Models with Model Registry.\n",
    "\n",
    "This step is passed into the condition step and only executes if the condition step evaluates to true.\n",
    "\n",
    "## To define a RegisterModel step to create a model package\n",
    "\n",
    "- Construct a RegisterModel step using the estimator instance you used for the training step . Pass in the S3ModelArtifacts property from the step_train training step and specify a ModelPackageGroup. SageMaker Pipelines creates this ModelPackageGroup for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3753a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics \n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\"\n",
    "    )\n",
    ")\n",
    "step_register = RegisterModel(\n",
    "    name=\"AbaloneRegisterModel\",\n",
    "    estimator=xgb_train,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e979f5",
   "metadata": {},
   "source": [
    "# Step 9: Define a Condition Step to Verify Model Accuracy\n",
    "A ConditionStep allows SageMaker Pipelines to support conditional execution in your pipeline DAG based on the condition of step properties. In this case, you only want to register a model package if the accuracy of that model, as determined by the model evaluation step, exceeds the required value. If the accuracy exceeds the required value, the pipeline also creates a SageMaker Model and runs batch transformation on a dataset. This section shows how to define the Condition step.\n",
    "\n",
    "## To define a condition step to verify model accuracy\n",
    "\n",
    "1. Define a ConditionLessThanOrEqualTo condition using the accuracy value found in the output of the model evaluation processing step, step_eval. Get this output using the property file you indexed in the processing step and the respective JSONPath of the mean squared error value, \"mse\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8dc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "\n",
    "cond_lte = ConditionLessThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=step_eval.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"regression_metrics.mse.value\"\n",
    "    ),\n",
    "    right=6.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a531f360",
   "metadata": {},
   "source": [
    "2. Construct a ConditionStep. Pass the ConditionEquals condition in, then set the model package registration and batch transformation steps as the next steps if the condition passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7805de",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_cond = ConditionStep(\n",
    "    name=\"AbaloneMSECond\",\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[step_register, step_create_model, step_transform],\n",
    "    else_steps=[], \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3467e2b8",
   "metadata": {},
   "source": [
    "# Step 10: Create a pipeline\n",
    "Now that you’ve created all of the steps, combine them into a pipeline.\n",
    "\n",
    "To create a pipeline\n",
    "\n",
    "1. Define the following for your pipeline: name, parameters, and steps. Names must be unique within an (account, region) pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4cacc0",
   "metadata": {},
   "source": [
    "> Note:\n",
    " A step can only appear once in either the pipeline's step list or the if/else step lists of the condition step. It cannot appear in both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dae776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "\n",
    "pipeline_name = f\"AbalonePipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "        batch_data,\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_eval, step_cond],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d2cd45",
   "metadata": {},
   "source": [
    "This pipeline definition is ready to submit to SageMaker. In the next tutorial, you submit this pipeline to SageMaker and start an execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc4350b",
   "metadata": {},
   "source": [
    "# **Run a pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75482775",
   "metadata": {},
   "source": [
    "# Step 1: Start the Pipeline\n",
    "First, you need to start the pipeline.\n",
    "\n",
    "To start the pipeline\n",
    "\n",
    "1. Examine the JSON pipeline definition to ensure that it's well-formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8646dd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.loads(pipeline.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6843bf53",
   "metadata": {},
   "source": [
    "2. Submit the pipeline definition to the SageMaker Pipelines service to create a pipeline if it doesn't exist, or update the pipeline if it does. The role passed in is used by SageMaker Pipelines to create all of the jobs defined in the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12e79c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2889e202",
   "metadata": {},
   "source": [
    "3. Start a pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0077b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51173f86",
   "metadata": {},
   "source": [
    "# Step 2: Examine a Pipeline Execution\n",
    "Next, you need to examine the pipeline execution.\n",
    "\n",
    "## To examine a pipeline execution\n",
    "\n",
    "1. Describe the pipeline execution status to ensure that it has been created and started successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8610a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5e9e52",
   "metadata": {},
   "source": [
    "2. Wait for the execution to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deead36",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9011d81",
   "metadata": {},
   "source": [
    "3. List the execution steps and their status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433979de",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d242027",
   "metadata": {},
   "source": [
    "4. After your pipeline execution is complete, download the resulting  evaluation.json file from Amazon S3 to examine the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42b44c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_json = sagemaker.s3.S3Downloader.read_file(\"{}/evaluation.json\".format(\n",
    "    step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "))\n",
    "json.loads(evaluation_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c826a",
   "metadata": {},
   "source": [
    "# Step 3: Override Default Parameters for a Pipeline Execution\n",
    "You can run additional executions of the pipeline by specifying different pipeline parameters to override the defaults.\n",
    "\n",
    "## To override default parameters\n",
    "\n",
    "1. Create the pipeline execution. This starts another pipeline execution with the model approval status override set to \"Approved\". This means that the model package version generated by the RegisterModel step is automatically ready for deployment through CI/CD pipelines, such as with SageMaker Projects. For more information, see Automate MLOps with SageMaker Projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366d66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start(\n",
    "    parameters=dict(\n",
    "        ModelApprovalStatus=\"Approved\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cb4626",
   "metadata": {},
   "source": [
    "2. Wait for the execution to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c66b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdb9f7c",
   "metadata": {},
   "source": [
    "3. List the execution steps and their status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa716e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fad617",
   "metadata": {},
   "source": [
    "4. After your pipeline execution is complete, download the resulting  evaluation.json file from Amazon S3 to examine the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c00b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_json = sagemaker.s3.S3Downloader.read_file(\"{}/evaluation.json\".format(\n",
    "    step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "))\n",
    "json.loads(evaluation_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47d5fef",
   "metadata": {},
   "source": [
    "# Step 4: Stop and Delete a Pipeline Execution\n",
    "When you're finished with your pipeline, you can stop any ongoing executions and delete the pipeline.\n",
    "\n",
    "## To stop and delete a pipeline execution\n",
    "\n",
    "1. Stop the pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04731b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a7cc0",
   "metadata": {},
   "source": [
    "2. Delete the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5645067",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
